这是一个非常深刻的问题，触及了 EMMA 以及现代“跨具身智能（Cross-Embodiment Learning）”的核心机制。

EMMA 并不是通过简单的“把人手映射成机械臂”来学习操作的（这种显式映射通常因为自由度差异太大而失败）。它使用的是一种**协同训练（Co-training）**策略，通过共享神经网络的中间层来实现知识迁移。

以下是关于 Manipulation（操作）部分的详细拆解：

### 1. 数据的角色：人类 vs. 机器人

在 EMMA 中，这两类数据的作用截然不同，但互为补充：

*   **人类数据 ($D_H$)：**
    *   **形式：** Aria 眼镜的第一人称视频 + **人手的 6DoF 位姿** (位置+旋转，由 Aria MPS 算法提取的 Keypoints 转换而来)。
    *   **作用：** 提供“大脑”和“通用性”。人类数据量大、场景多，它教会模型**“在什么场景下应该做什么”**（例如：看到桌子上有个杯子，应该伸手去拿；看到人伸手，应该把东西递过去）。
*   **机器人数据 ($D_R$)：**
    *   **形式：** 机器人第一人称视频 + 腕部相机视频 + **机械臂关节角度 & 末端执行器位姿**。
    *   **作用：** 提供“小脑”和“执行力”。它教会模型**“具体的电机该怎么转”**才能实现那个意图（例如：为了把夹爪送到那个位置，ViperX 机械臂的6个关节应该怎么配合）。

### 2. 数据对齐 (Alignment)

在送入神经网络之前，必须先把两者“拉”到同一个对话频道上：

1.  **坐标系统一：** 无论是人手还是机械臂末端，所有的位置信息都被转换到**当前时刻的以自我为中心的相机坐标系（Egocentric Camera Frame）**下。
    *   这意味着模型不需要知道“我在世界的哪里”，只需要知道“手相对于眼镜在哪里”。这消除了人身高和机器人身高不同的影响。
2.  **归一化 (Z-Score Normalization)：** 人手的活动范围和速度与机械臂不同。作者分别对人类数据和机器人数据进行标准化处理，使它们的数值分布接近，方便神经网络同时处理。

### 3. 网络架构：Stem-Trunk-Head (茎-干-头)

EMMA 使用了基于 Transformer 的架构（类似于 HPT 或 Octo），其设计巧妙地处理了异构数据：

*   **输入茎 (Stems - 独立)：**
    *   **Proprio Stem (本体感知茎)：** 有两个独立的网络。一个专门编码人手状态，一个专门编码机器人关节状态。
    *   **Vision Stem (视觉茎 - **核心共享**)：** 这是**连接人类和机器人的桥梁**。处理 Aria 眼镜 RGB 图像的编码器（如 ResNet/ViT）是**共享权重**的。这意味着无论输入是人看到的还是机器人看到的，模型被迫提取通用的视觉特征。
*   **主干 (Trunk - **核心共享**)：**
    *   这是一个 Transformer，处理来自 Stems 的 Token。它学习的是任务的时序逻辑和决策。因为权重共享，它从人类数据中学到了“策略”，从机器人数据中学到了“物理约束”。
*   **输出头 (Heads - 独立)：**
    *   **Human Action Head:** 预测人手未来的位姿。
    *   **Robot Action Head:** 预测机器人未来的关节角度/夹爪动作。

---

### 4. 训练流程 (Training Pipeline)

训练时，每一个 Batch（批次）要么全是人类数据，要么全是机器人数据，交替进行：

*   **当输入是人类数据 Batch 时：**
    1.  图像通过共享 Vision Stem。
    2.  人手状态通过 Human Proprio Stem。
    3.  两者进入共享 Trunk。
    4.  输出通过 **Human Action Head**。
    5.  **Loss 计算：** 预测的人手位姿 vs. 真实人手位姿。
    6.  **梯度更新：** 更新 Vision Stem, Trunk, 和 Human Head。

*   **当输入是机器人数据 Batch 时：**
    1.  图像通过共享 Vision Stem。
    2.  机器人状态通过 Robot Proprio Stem。
    3.  两者进入共享 Trunk。
    4.  输出通过 **Robot Action Head**。
    5.  **Loss 计算：** 预测的关节角度 vs. 真实关节角度。
    6.  **梯度更新：** 更新 Vision Stem, Trunk, 和 Robot Head。

**魔法发生的地方：**
虽然 Robot Action Head 从未见过人类数据，但因为它连接的 **Vision Stem 和 Trunk** 是通过大量人类数据训练过的（见多识广），所以当机器人处于一个新场景（这个场景只在人类数据里出现过）时，Vision Stem 能正确提取特征，Trunk 能正确规划意图，最后 Robot Head 只需要负责把这个意图“翻译”成机械臂动作即可。

---

### 5. 推理流程 (Inference Pipeline)

在机器人实际部署时（Deployment）：

1.  **输入：** 机器人实时的 Aria 视频流 + 机器人当前的关节状态。
2.  **前向传播：**
    *   视频 -> **Shared** Vision Stem
    *   关节 -> **Robot** Proprio Stem
    *   特征融合 -> **Shared** Trunk
    *   解码 -> **Robot** Action Head
3.  **输出：** 机器人关节的控制指令。
4.  **执行：** 机器人执行动作。

**注意：** 在推理时，**Human Stems 和 Human Heads 被完全丢弃了**。它们的作用仅仅是在训练阶段作为“脚手架”，帮助共享网络（Trunk/Vision）学习到更好的特征和策略。

### 总结

EMMA 学会操作的关键在于：**视觉和决策逻辑（Vision & Trunk）是通用的，而本体感知和执行（Proprio & Heads）是特异的。**

*   人类数据教共享部分：“看到那个杯子，我们要去接近它。”
*   机器人数据教特异部分：“如果要接近它，你的肘关节要转30度。”

这就是为什么只用少量的机器人数据（学会如何动）加上大量的人类数据（学会各种场景怎么应对），就能训练出强大的移动操作策略。